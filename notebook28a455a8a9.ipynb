{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem 2 - Transformers for Language Modeling and Sentiment Analysis\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/csci566-spring2022/csci566-assignment2/blob/master/CSCI566_Assignment2_problem_2_Transformer.ipynb)\n\nIn this problem we will learn how to implement the building blocks for \"Transformer\" models, and then implement a pre-training procedure for such models via [BERT](https://arxiv.org/abs/1810.04805)-style language modeling and then fine-tune a pre-trained model on sentiment analysis tasks on the IMDB movie review dataset.\nTypically, transformer models are very large and are pre-trained on language modeling tasks with massive datasets with huge computational resources.\nAs such, we will only implement the pre-training *procedure*, without expecting you to pre-train a model to completion.\nWe will then load in a pre-trained model for you to perform fine-tuning on a sentiment analysis task.\n\nWe will complete the following steps in this problem:\n1. Implement a multi-head-attention (MHA) layer.\n1. Implement \"Transformer block\" layers which use MHA layers, linear layers, and residual connections.\n1. Implement a full Transformer model comprised of Transformer blocks.\n1. Implement [BERT](https://arxiv.org/abs/1810.04805)-style language model pre-training for the Transformer model.\n1. Fine-tune our trained language model on a sentiment analysis task.\n\nIn order to run on GPU in Colab go to `Runtime -> Change runtime type` and select `GPU` under the `Hardware accelerator` drop-down box.","metadata":{"id":"whiS8A4lblMy"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport math\nimport random\nimport numpy as np\n\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"a2rOZ3dEblM0","executionInfo":{"status":"ok","timestamp":1648314737719,"user_tz":420,"elapsed":8520,"user":{"displayName":"","photoUrl":"","userId":""}},"execution":{"iopub.status.busy":"2022-03-27T06:19:05.609089Z","iopub.execute_input":"2022-03-27T06:19:05.609473Z","iopub.status.idle":"2022-03-27T06:19:05.615930Z","shell.execute_reply.started":"2022-03-27T06:19:05.609439Z","shell.execute_reply":"2022-03-27T06:19:05.615228Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Scaled Dot Product Attention [8 points]\nThe attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. Here we use the following definition: *the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys.* In other words, we want to dynamically decide on which inputs we want to “attend” more than others based on their values. In particular, an attention mechanism has usually **4 parts** we need to specify:\n\n\n*   **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n*   **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n*   **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n*   **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n\nThe weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query.\n\nThe attention applied inside the [Transformer](https://arxiv.org/abs/1706.03762) architecture is called self-attention. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements’ keys, and returned a different, averaged value vector for each element.\n\nThe core concept behind self-attention is the scaled dot product attention. The dot product attention takes as input a set of queries $Q \\in \\mathbb{R}^{T \\times d_k}$, keys $K \\in \\mathbb{R}^{T \\times d_k}$ and values $V \\in \\mathbb{R}^{T \\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively.\nThe attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. Mathmatically:\n\n$$Attention(Q,K,V)=\\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\n\nThe matrix multiplication $Q K^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T \\times T$. \nEach row represents the attention logits for a specific element $i$ to all other elements in the sequence. \nWe apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). \nThe computation graph is visualized below.\n\n<p align=\"center\">\n<img src=\"https://production-media.paperswithcode.com/methods/SCALDE.png\" alt=\"drawing\" width=\"300\"/>\n</p>","metadata":{"id":"DeYN4z2V-_F2"}},{"cell_type":"code","source":"################################### TODO ###################################\n# Implement the scaled dot product attention described above.\n############################################################################\ndef scaled_dot_product(q, k, v, attn_drop_rate=0.1, mask=None):\n    \"\"\"\n    Parameters:\n      q: query, shape: (batch, # heads, seq len, head dimension)\n      k: keys, shape: (batch, # heads, seq len, head dimension)\n      v: value, shape: (batch, # heads, seq len, head dimension)\n      attn_drop_rate: probability of an element to be zeroed,\n      mask: the optional masking of specific entries in the attention matrix.\n              shape: (batch, seq len)\n    \"\"\"\n    # TODO: get hidden dimensionality d_k for query/keys.\n    d_k = q.shape[-1]\n\n    # TODO: compute (QK^T)/d_k, use https://pytorch.org/docs/stable/generated/torch.matmul.html.\n    attn_logits = torch.matmul(q,torch.transpose(k,2,3))/np.sqrt(d_k)\n\n    # TODO: if mask is not None, apply mask. use https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_.\n    # Make sure that padding tokens cannot be attended to by subtracting a\n    # large negative value from the columns of attention weights\n    # corresponding to the tokens that have mask = 1. These will become 0\n    # after the softmax.\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill_(mask.reshape(mask.shape[0],1,1,-1)==1, -math.inf)\n    \n    \n    # TODO: compute softmax((QK^T)/d_k). Normalize attention weights to sum to 1 with a softmax over the key dimension.\n    attention = nn.Softmax(dim=-1)(attn_logits)\n\n    # TODO: Add dropout to attention weights w/ attn_drop_rate.\n    attention = torch.nn.Dropout(p=attn_drop_rate)(attention)\n\n    # TODO: compute softmax((QK^T)/d_k)V.\n    values = torch.matmul(attention, v)\n\n    return values, attention","metadata":{"id":"Uy-k61ik_Dg5","executionInfo":{"status":"ok","timestamp":1648267299525,"user_tz":420,"elapsed":155,"user":{"displayName":"","photoUrl":"","userId":""}},"execution":{"iopub.status.busy":"2022-03-27T06:19:08.721101Z","iopub.execute_input":"2022-03-27T06:19:08.721780Z","iopub.status.idle":"2022-03-27T06:19:08.731116Z","shell.execute_reply.started":"2022-03-27T06:19:08.721744Z","shell.execute_reply":"2022-03-27T06:19:08.730413Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Before you continue, run the test code listed below. It will generate random queries, keys, and value vectors, and calculate the attention outputs. Make sure you can follow the calculation of the specific values here, and also check it by hand. ","metadata":{"id":"yTQd7kt6DA48"}},{"cell_type":"code","source":"bs = 1\nnum_heads = 1\nseq_len, d_k = 3, 2\nq = torch.randn(bs, num_heads, seq_len, d_k)\nk = torch.randn(bs, num_heads, seq_len, d_k)\nv = torch.randn(bs, num_heads, seq_len, d_k)\nmask = torch.bernoulli(0.5 * torch.ones(bs, seq_len))\nvalues, attention = scaled_dot_product(q, k, v, 0.0, mask)\nprint(\"Q\\n\", q)\nprint(\"K\\n\", k)\nprint(\"V\\n\", v)\nprint(\"Mask\\n\", mask)\nprint(\"Values\\n\", values)\nprint(\"Attention\\n\", attention)","metadata":{"id":"78hBTlzRDJeL","outputId":"a6255374-bb69-4484-e8d2-3d9f73bdf45f","executionInfo":{"status":"ok","timestamp":1648267713628,"user_tz":420,"elapsed":136,"user":{"displayName":"","photoUrl":"","userId":""}},"execution":{"iopub.status.busy":"2022-03-27T06:19:10.922675Z","iopub.execute_input":"2022-03-27T06:19:10.923436Z","iopub.status.idle":"2022-03-27T06:19:11.014932Z","shell.execute_reply.started":"2022-03-27T06:19:10.923382Z","shell.execute_reply":"2022-03-27T06:19:11.014155Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Q\n tensor([[[[ 0.0461,  0.4024],\n          [-1.0115,  0.2167],\n          [-0.6123,  0.5036]]]])\nK\n tensor([[[[ 0.2310,  0.6931],\n          [-0.2669,  2.1785],\n          [ 0.1021, -0.2590]]]])\nV\n tensor([[[[-0.1549, -1.3706],\n          [-0.1319,  0.8848],\n          [-0.2611,  0.6104]]]])\nMask\n tensor([[0., 1., 0.]])\nValues\n tensor([[[[-0.2007, -0.5155],\n          [-0.2066, -0.4067],\n          [-0.2005, -0.5194]]]])\nAttention\n tensor([[[[0.5683, 0.0000, 0.4317],\n          [0.5134, 0.0000, 0.4866],\n          [0.5703, 0.0000, 0.4297]]]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2 - Build Multi-Head-Attention Layer [8 points]\n\nNow we will implement multi-head-attention, first introduced by [Attention is All you Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762).\nThe scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features.\n<!-- \"Attention\" is a computational mechanism which allows models to selectively weight different tokens in other parts of the input. -->\n<!-- For example, given the sentence, \"John went to the dentist to get his teeth cleaned.\" the model might learn to use the \"his\" token to attend to the \"John\" token, as the word \"his\" is referring to \"John\" in this context. -->\n\nA multi-head-attention layer works by employing several self-attention layers in parallel.\nGiven a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently where $h$ is the number of heads.\nAfterward, we concatenate the heads and combine them with a final weight matrix.\nMathmatically,\n\n$$Multihead(Q,K,V)=Concat(head_1, ..., head_h)W^O,$$\n\nwhere\n\n$$head_i=Attention(QW^Q_i, KW^K_i, VW^V_i).$$\n\nWe refer to this as Multi-Head Attention layer with the learnable parameters $W^Q_{1...h}\\in \\mathbb{R}^{d_{in}\\times d_k}$, $W^K_{1...h}\\in \\mathbb{R}^{d_{in}\\times d_k}$, $W^V_{1...h}\\in \\mathbb{R}^{d_{in}\\times d_v}$, and $W^O\\in \\mathbb{R}^{h\\cdot d_k \\times d_{out}}$ where $d_{in}$ is the input dimensionality, and $d_{out}$ is the output dimensionality. \nThe visualized computational graph is shown below.\n\n<p align=\"center\">\n<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" alt=\"drawing\" width=\"300\"/>\n</p>\n\nLooking at the computation graph above, a simple but effective implementation is to set the current feature map $X$ in a NN, $X\\in \\mathbb{R}^{B\\times T\\times d_{model}}$, where $B$ is the batch size, $T$ is the sequence length, and $d_{model}$ is the hidden dimentionality of $X$.\n<!-- Attention works by computing three vectors for each input vector (e.g. embedded token): Query, Key, and Value.\nThey can be computed via a fully connected layer.\nBelow is a diagram of the multi-head attention layer.\n\n![](https://miro.medium.com/max/1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)\n\nWe can represent scaled Dot-Product Attention with the following equation (assuming $Q, K, V \\in \\mathbb{R}^{t \\times h}$ where $t$ is the length of the sequence and $h$ is the dimensionality of the attention head): -->\n<!-- \nThe result of the left-hand side of this equation ($\\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{h}}\\right)$) is a matrix of size $t \\times t$ where every row sums to one.\nThe element in row $i$ and column $j$ represents how much the token at position $i$ in the sequence is attending to the token at position $j$.\nMulti-Head Attention computes several Dot-Product Attentions in parallel and concatenates the outputs.\nTypically we specify the dimensionality of the full layer $d$ as well as the number of heads $n$ and then set the dimensionality of each head to be $h = \\frac{d}{n}$. -->","metadata":{"id":"LrdVJlTd64Uw"}},{"cell_type":"code","source":"################################### TODO ###################################\n# Implement Multi-head attention described above.\n############################################################################\nclass MultiHeadAttention(nn.Module):\n  def __init__(self, embed_dim, n_heads, attn_drop_rate):\n    \"\"\"\n    Parameters:\n      input_dim: The input dimension.\n      embed_dim: The embedding dimension of the model\n      n_heads: Number of attention heads\n      attn_drop_rate: Dropout rate for attention weights (Q K^T)\n    \"\"\"\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.n_heads = n_heads\n    self.head_dim = embed_dim // n_heads\n    self.attn_drop_rate = attn_drop_rate\n\n    # TODO: Add learnable parameters for computing query, key, and value using nn.Linear. \n    # Store all weight matrices W^Q, W^K, W^V 1...h together for efficiency.\n    self.qkv_proj = nn.Linear(embed_dim,3*embed_dim)\n\n    # TODO: Add learnable parameters W^O using nn.Linear.\n    self.o_proj = nn.Linear(embed_dim,embed_dim)\n\n    self._reset_parameters()\n\n  def _reset_parameters(self):\n      # Original Transformer initialization, see PyTorch documentation\n      nn.init.xavier_uniform_(self.qkv_proj.weight)\n      self.qkv_proj.bias.data.fill_(0)\n      nn.init.xavier_uniform_(self.o_proj.weight)\n      self.o_proj.bias.data.fill_(0)\n  \n  def forward(self, embedding, mask):\n    \"\"\"\n    Inputs:\n      embedding: Input embedding with shape (batch size, sequence length, embedding dimension)\n      mask: Mask specifying padding tokens with shape (batch_size, sequence length)\n        Value for tokens that should be masked out is 1 and 0 otherwise.\n    Outputs:\n      Attended values\n    \"\"\"\n    \n    # TODO: get batch_size, seq_length, embed_dim.\n    batch_size, seq_length, embed_dim = embedding.shape\n\n    # TODO: Compute queries, keys, and values (keep continguous for now).\n    qkv = self.qkv_proj(embedding)\n\n    # TODO: Separate Q, K, V from linear output, give each shape [batch, num_head, seq_len, head_dim] (may require transposing/permuting dimensions)\n    q, k, v = torch.transpose(qkv[:,:,:embed_dim].reshape(batch_size,seq_length,self.n_heads,self.head_dim),1,2), \\\n              torch.transpose(qkv[:,:,embed_dim:2*embed_dim].reshape(batch_size,seq_length,self.n_heads,self.head_dim),1,2), \\\n              torch.transpose(qkv[:,:,2*embed_dim:].reshape(batch_size,seq_length,self.n_heads,self.head_dim),1,2)\n\n    # TODO: Determine value outputs, with shape [batch, seq_len, num_head, head_dim]. (hint: use scaled_dot_product())\n    values, attention = scaled_dot_product(q, k, v, attn_drop_rate=self.attn_drop_rate, mask=mask)\n    values = torch.transpose(values,1,2)\n    \n    # TODO: Linearly project attention outputs w/ W^O.\n    # The final dimensionality should match that of the inputs.\n    attended_embeds = self.o_proj(values.reshape(batch_size,seq_length,-1))\n    \n    return attended_embeds","metadata":{"id":"Zvj40hQ-DcC1","execution":{"iopub.status.busy":"2022-03-27T06:07:40.415301Z","iopub.execute_input":"2022-03-27T06:07:40.415706Z","iopub.status.idle":"2022-03-27T06:07:40.428956Z","shell.execute_reply.started":"2022-03-27T06:07:40.415669Z","shell.execute_reply":"2022-03-27T06:07:40.428267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check that your MHA layer works and returns a tensor of the correct shape","metadata":{"id":"XMiKHd8Leo8r"}},{"cell_type":"code","source":"embed_dim = 16\nn_heads = 4\nattn_drop_rate = 0.1\nlayer = MultiHeadAttention(embed_dim, n_heads, attn_drop_rate)\n\nbs = 3\nseq_len = 2\ninputs = torch.randn(bs, seq_len, embed_dim)\nmask = torch.zeros(bs, seq_len)\noutputs = layer(inputs, mask)\nout_bs, out_seq_len, out_hidden = outputs.shape\nprint(\"Output shape: \", (out_bs, out_seq_len, out_hidden))\nassert out_bs == bs and out_seq_len == seq_len and out_hidden == embed_dim, \"Unexpected output shape\"","metadata":{"id":"NHxBmCwnEqrj","execution":{"iopub.status.busy":"2022-03-27T06:07:40.431995Z","iopub.execute_input":"2022-03-27T06:07:40.432715Z","iopub.status.idle":"2022-03-27T06:07:40.448584Z","shell.execute_reply.started":"2022-03-27T06:07:40.432678Z","shell.execute_reply":"2022-03-27T06:07:40.447861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Build Transformer Blocks [8 points]\n\nNow we construct the blocks from which transformer models are comprised of.\n\nOriginally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. \nThe visualized computational graph is shown below.\nHere we will mainly focus on the encoder part and implement the encoder block.\n\n<p align=\"center\">\n<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" width=\"300\"/>\n</p>\n\nA Transformer encoder block consists of the following modules in this order:\n1.   Multi-Head Attention (we implemented above)\n1.   Dropout\n1.   Residual connection to the input (simply add the input of the block to the output of the previous dropout layer).\n1.   Layer Norm - https://arxiv.org/abs/1607.06450\n1.   Linear layer\n1.   Activation function (typically gelu - https://arxiv.org/abs/1606.08415)\n1.   Linear layer\n1.   Dropout\n1.   Residual connection to 4  (add the output of 4 to 8)\n1.   Layer Norm\n\nAccording to the listed modules, please implement:\n\n```\nclass TransformerBlock(nn.Module)\n```\n\n\n\n","metadata":{"id":"UsZmxmA8760j"}},{"cell_type":"code","source":"################################### TODO ###################################\n# Implement transformer encoder block\n############################################################################\nclass TransformerBlock(nn.Module):\n  def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n    \"\"\"\n    Parameters:\n      input_dim: Dimensionality of the input\n      embed_dim: The embedding dimension of the model\n      n_heads: Number of attention heads\n      attn_drop_rate: Dropout rate for attention weights (Q K^T)\n      layer_drop_rate: Dropout rate for activations\n    \"\"\"\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.n_heads = n_heads\n    self.layer_dropout = nn.Dropout(layer_drop_rate)\n\n    # TODO: define attention layer\n    self.self_attn = MultiHeadAttention(embed_dim, n_heads, attn_drop_rate)\n\n    # TODO: define a network (using nn.Sequential) with: \n    # 1) a linear layer, 2) an activation layer, 3) another linear layer, 4) a dropout layer.\n    self.linear_net = nn.Sequential(\n                        nn.Linear(embed_dim,4*embed_dim),\n                        torch.nn.GELU(),\n                        nn.Linear(4*embed_dim,embed_dim),\n                        nn.Dropout(layer_drop_rate)\n                      )\n    \n    # TODO: define 2 norm layers, 1 dropout layer.\n    self.norm1 = nn.LayerNorm(embed_dim)\n    self.norm2 = nn.LayerNorm(embed_dim)\n    self.dropout = nn.Dropout(p=layer_drop_rate)\n\n  def forward(self, inputs):\n    embedding, mask = inputs\n\n    # TODO: 1. compute multi-head attention\n    attn_out = self.self_attn(embedding, mask)\n\n    # TODO: 2. add dropout\n    dropout_out = self.dropout(attn_out)\n\n    # TODO: 3. add residual connection to the input\n    embedding = dropout_out + embedding\n\n    # TODO: 4. apply layernorm\n    embedding = self.norm1(embedding)\n\n    # TODO: 5-8. compute 1) a linear layer, 2) an activation layer, 3) another linear layer, 4) a dropout layer.\n    linear_out = self.linear_net(embedding)\n\n    # TODO: 9. add residual connection \n    embedding = embedding + linear_out\n\n    # TODO: 10. apply layer norm\n    embedding = self.norm2(embedding)\n\n    return embedding, mask","metadata":{"id":"PnGkMpbCFTXC","execution":{"iopub.status.busy":"2022-03-27T06:07:40.451799Z","iopub.execute_input":"2022-03-27T06:07:40.452008Z","iopub.status.idle":"2022-03-27T06:07:40.464048Z","shell.execute_reply.started":"2022-03-27T06:07:40.451985Z","shell.execute_reply":"2022-03-27T06:07:40.463146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's once again check that the code runs without error and outputs the correct shape (note, this is not a guarantee that you have implemented it correctly).","metadata":{"id":"mUeAV4qSUnJu"}},{"cell_type":"code","source":"embed_dim = 16\nn_heads = 4\nattn_drop_rate = 0.1\nlayer_drop_rate = 0.1\nblock = TransformerBlock(embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n\nbs = 3\nseq_len = 2\nembeds = torch.randn(bs, seq_len, embed_dim)\nmask = torch.zeros(bs, seq_len)\noutputs, _ = block((embeds, mask))\nout_bs, out_seq_len, out_hidden = outputs.shape\nprint(\"Output shape: \", (out_bs, out_seq_len, out_hidden))\nassert out_bs == bs and out_seq_len == seq_len and out_hidden == embed_dim, \"Unexpected output shape\"","metadata":{"id":"6wvM7EPGSq8e","execution":{"iopub.status.busy":"2022-03-27T06:07:40.466803Z","iopub.execute_input":"2022-03-27T06:07:40.467268Z","iopub.status.idle":"2022-03-27T06:07:40.481736Z","shell.execute_reply.started":"2022-03-27T06:07:40.467221Z","shell.execute_reply":"2022-03-27T06:07:40.480955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 - Position Encoding [0 points]\n\nIn tasks like language understanding, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences.\nMathmatically:\n\n$$PE(pos, i) = \\left\\{\\begin{matrix}\n\\sin (\\frac{pos}{10000^{i/d_{model}}}) & \\text{if } i\\mod 2=0\\\\ \n\\cos (\\frac{pos}{10000^{(i-1)/d_{model}}}) & \\text{otherwise} \n\\end{matrix}\\right.$$\n\n$PE(pos,i)$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$.\nThese values, concatenated for all hidden dimensions, are added to the original input features, and constitute the position information.\nThe intuition behind this encoding is that you can represent $PE(pos+k,:)$ as a linear function of $PE(pos,:)$, which might allow the model to easily attend to relative positions.","metadata":{"id":"BHBvnmRL76qa"}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=drop_rate)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n        pe = torch.zeros(1, max_len, embed_dim)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"id":"qf-vzofmF6yy","execution":{"iopub.status.busy":"2022-03-27T06:07:40.483514Z","iopub.execute_input":"2022-03-27T06:07:40.484089Z","iopub.status.idle":"2022-03-27T06:07:40.493798Z","shell.execute_reply.started":"2022-03-27T06:07:40.484042Z","shell.execute_reply":"2022-03-27T06:07:40.493059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 - Build a BERT model [8 points]\n A BERT model consists of:\n\n1.   **An input embedding layer.** This converts a token index into a vector embedding. Make sure to include an extra embedding for the masked tokens! In other words, learn vocab_size + 1 embeddings.\n2.   **Positional encodings.** This layer (implemented for you already) encodes the position of each token since multi-head-attention layers have no notion of positional locality or order. It takes as input the the token embeddings from (1) and returns them with positional embeddings added.\n3.   Several stacked **Transformer blocks** (the number specified by n_layers)\n4.   **Output linear layer** that predicts masked words for pre-training. Takes final embedding of last block and outputs probability distribution over the vocabulary.","metadata":{"id":"g9HAPz85gEGc"}},{"cell_type":"code","source":"################################### TODO ###################################\n# Add the requisite modules for a BERT model\n############################################################################\nclass BertModel(nn.Module):\n  def __init__(self, n_layers, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n    super().__init__()\n\n    # TODO: 1. add input embedding layer (hint: use nn.Embedding) - don't forget about the mask token\n    self.embed = nn.Embedding(vocab_size+1, embed_dim)\n    \n    # TODO: 2. add positional encoding\n    self.pos_embed = PositionalEncoding(embed_dim, drop_rate=layer_drop_rate)\n\n    # TODO: 3. add stacked transformer blocks (use nn.Sequential)\n    layers = []\n    for _ in range(n_layers):\n        layers.append(TransformerBlock(embed_dim, n_heads, attn_drop_rate, layer_drop_rate))\n    self.net = nn.Sequential(*layers)\n    \n    # TODO: 4. add output linear layer that predicts masked words for pre-training\n    self.mask_pred = nn.Linear(embed_dim,vocab_size)\n\n  def forward(self, batch_text, mask=None):\n    # TODO: implement forward pass (embedding -> stacked blocks -> output masked word predictions)\n    embed = self.embed(batch_text)\n    pos = self.pos_embed(embed)\n    output,_ = self.net((pos,mask))\n    mask_preds = self.mask_pred(output)\n    return mask_preds","metadata":{"id":"-cTFKj5GgKFf","execution":{"iopub.status.busy":"2022-03-27T06:07:40.495806Z","iopub.execute_input":"2022-03-27T06:07:40.496586Z","iopub.status.idle":"2022-03-27T06:07:40.507458Z","shell.execute_reply.started":"2022-03-27T06:07:40.496465Z","shell.execute_reply":"2022-03-27T06:07:40.506567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's once again check that the code runs without error and outputs the correct shape (note, this is not a guarantee that you have implemented it correctly).","metadata":{"id":"Je1o9NpamV8d"}},{"cell_type":"code","source":"embed_dim = 16\nn_heads = 4\nn_layers = 2\nvocab_size = 10\nattn_drop_rate = 0.1\nlayer_drop_rate = 0.1\nmodel = BertModel(n_layers, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n\nbs = 3\nseq_len = 2\ninputs = torch.randint(0, vocab_size, (bs, seq_len))\nmask_preds = model(inputs)\nout_bs, out_seq_len, out_vocab = mask_preds.shape\nprint(\"Mask predictions shape: \", (out_bs, out_seq_len, out_vocab))\nassert out_bs == bs and out_seq_len == seq_len and out_vocab == vocab_size, \"Unexpected mask prediction output shape\"","metadata":{"id":"BnLLpy33mV8n","execution":{"iopub.status.busy":"2022-03-27T06:07:40.508958Z","iopub.execute_input":"2022-03-27T06:07:40.509544Z","iopub.status.idle":"2022-03-27T06:07:40.527408Z","shell.execute_reply.started":"2022-03-27T06:07:40.509506Z","shell.execute_reply":"2022-03-27T06:07:40.526714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 6 - Implement BERT Pre-Training [8 points]\n\nIn order to pre-train our language model, we randomly permute `mask_rate`% of the tokens and attempt to predict the original tokens.\nThe permutation is as follows:\n* In 80% of these cases we replace the token with a `<mask>` token. Use `MASK_TOKEN_IND` as the index of this token.\n* In 10% of these cases we replace the token with a random token.\n* In the final 10% we do not permute the token.\n\nThe prediction task is then to predict the original token for *only* the permuted tokens.\nYou should use `nn.CrossEntropyLoss`.\nNote that this module has a keyword argument `ignore_index` which specifies a label index for which we do not compute the loss.\nIt is `-100` by default.\nThis can be used to **only** do prediction for the permuted tokens.\n\nFor more details, please look at Task 1 in Section 3.1 of the [BERT paper](https://arxiv.org/abs/1810.04805).\nWe do not consider the second pre-training task (Next Sentence Prediction) for this assignment.\n\n**We do not expect you to complete the pre-training procedure, which is not feasible given your computational resources. We are simply asking you to implement one step of training with synthetic data.**","metadata":{"id":"HUCtAVSa76hQ"}},{"cell_type":"code","source":"batch_size = 128\nlearning_rate = 1e-4\nn_layers = 2  # number of transformer blocks in model\nembed_dim = 64\nn_heads = 4\nattn_drop_rate = 0.1  # dropout rate on attention weights\nlayer_drop_rate = 0.1  # dropout rate on activations\n\nmask_rate = 0.15  # rate at which we permute words in order to predict them\nvocab_size = 100\nMASK_TOKEN_IND = vocab_size\nPAD_IND = 0\nmodel = BertModel(n_layers, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\nopt = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.to(device)\nmodel.train()\n\ndef mask_inputs(text, only_mask=False):\n  \"\"\"\n  Inputs:\n    text: Batch of sequences of shape (batch_size, seq_len) and type torch.Long\n          Each token is represented by its index in the vocabulary.\n    only_mask: If this is true, only replace tokens with <mask> tokens, no\n               random tokens, or keeping tokens the same. This is used for\n               evaluation only.\n  Outputs:\n    masked_text: Permuted inputs based on rules defined in description above.\n    mask_labels: Labels for prediction. Use label -100 for tokens that we do not\n                 want to predict. Should have the same shape as input text.\n\n  \"\"\"\n  masked_text = text.clone()\n  mask_labels = text.clone()\n  ################################### TODO ###################################\n  # Implement random permutation of tokens based on mask_rate, store the masked\n  # sequences in masked_text. Note, you have access to mask_rate,\n  # MASK_TOKEN_IND, etc. inside this function. Also store the prediction labels\n  # for the pre-training task in mask_labels. Make sure to set the labels for\n  # non-permuted tokens as well as padding tokens to -100\n  ############################################################################\n  mask_labels[:,:] = -100\n  batch_size, seq_len = text.shape\n  mask = np.zeros((batch_size, seq_len))\n  mask[:,:round(mask_rate*seq_len)] = 1\n  for x in mask: np.random.shuffle(x)\n  for i in range(batch_size):\n    for j in range(seq_len):\n        if mask[i][j] == 1 and text[i][j]!=PAD_IND:\n            mask_labels[i][j] = masked_text[i][j]\n            p = random.random()\n            if p <0.8:\n                masked_text[i][j] = MASK_TOKEN_IND\n            elif p <0.9:\n                masked_text[i][j] = random.randrange(MASK_TOKEN_IND)\n  ################################ END TODO ##################################\n  return masked_text, mask_labels\n\ntext = torch.randint(1, vocab_size, (batch_size, 128)).to(device)\npad_mask = (text == PAD_IND).to(torch.uint8).to(device)  # this is a different type of mask (used to prevent attending to padding tokens)\nmasked_text, mask_labels = mask_inputs(text)\n\nchanged = (text != masked_text)\nmasked = (masked_text == MASK_TOKEN_IND)\nprint(\"Proportion of text changed (should be around 0.135): \", changed.float().mean().cpu().item())\nprint(\"Proportion of text masked (should be around 0.12): \", masked.float().mean().cpu().item())\n\nlabeled = (mask_labels != -100)\nprint(\"Proportion of data labeled for pre-training (should be around 0.15)\", labeled.float().mean().cpu().item())\n\nmask_preds = model(masked_text, pad_mask)\nloss_fn = nn.CrossEntropyLoss()\nloss = loss_fn(mask_preds.reshape((-1, vocab_size)), mask_labels.flatten())\nopt.zero_grad()\nloss.backward()\nopt.step()\nprint(\"Training step successfully completed! Loss value (should be around 4.6): \", loss.cpu().item())","metadata":{"id":"g59srQmteZvJ","execution":{"iopub.status.busy":"2022-03-27T06:07:40.528769Z","iopub.execute_input":"2022-03-27T06:07:40.529239Z","iopub.status.idle":"2022-03-27T06:07:42.711303Z","shell.execute_reply.started":"2022-03-27T06:07:40.529204Z","shell.execute_reply":"2022-03-27T06:07:42.710509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7 - Fine-Tune Pre-Trained Model on Sentiment Analysis [8 points]\n\nIn the previous section we implemented the pre-training procedure specified in the [BERT paper](https://arxiv.org/abs/1810.04805).\nNow, we will take a fully-trained BERT model and use its learned representations for performing a sentiment analysis task.\n\nWe will use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers.\nWe will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer.\nIn this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations.\n\nThe goal of this sentiment analysis task is to predict the \"sentiment\" of a particular sequence.\nIn this case the sequences are movie reviews are we're predicting whether they are positive or negative. Our model outputs a probability of positive sentiment for each input sequence. Use `nn.BCEWithLogitsLoss` to fine-tune the model on this task.\n\n## Preparing Data\n\nThe transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n\nLuckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained `bert-base-uncased` tokenizer.\n","metadata":{"id":"Ke6au928duGd"}},{"cell_type":"code","source":"!pip install transformers\n\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"id":"wvKvIOD7eyG4","execution":{"iopub.status.busy":"2022-03-27T06:07:42.712648Z","iopub.execute_input":"2022-03-27T06:07:42.713107Z","iopub.status.idle":"2022-03-27T06:07:50.903945Z","shell.execute_reply.started":"2022-03-27T06:07:42.713069Z","shell.execute_reply":"2022-03-27T06:07:50.903172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set constants regarding text tokenization and processing such that we are consistent with how the model was trained.","metadata":{"id":"t5REQmPW92eE"}},{"cell_type":"code","source":"init_token_idx = tokenizer.cls_token_id\neos_token_idx = tokenizer.sep_token_id\npad_token_idx = tokenizer.pad_token_id\nunk_token_idx = tokenizer.unk_token_id\nmax_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']","metadata":{"id":"TTISytZIfq5v","execution":{"iopub.status.busy":"2022-03-27T06:07:50.907375Z","iopub.execute_input":"2022-03-27T06:07:50.907871Z","iopub.status.idle":"2022-03-27T06:07:50.913189Z","shell.execute_reply.started":"2022-03-27T06:07:50.907819Z","shell.execute_reply":"2022-03-27T06:07:50.912112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define tokenization functions and set up IMDB dataset","metadata":{"id":"DwD9DCJJgCxS"}},{"cell_type":"code","source":"def tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence) \n    tokens = tokens[:max_input_length-2]\n    return tokens\n\nfrom torchtext.legacy import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = tokenize_and_cut,\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  init_token = init_token_idx,\n                  eos_token = eos_token_idx,\n                  pad_token = pad_token_idx,\n                  unk_token = unk_token_idx)\n\nLABEL = data.LabelField(dtype = torch.float)\n\nfrom torchtext.legacy import datasets\n\ntrain_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n\ntrain_data, valid_data = train_data.split(random_state = random.seed(SEED))\n\nLABEL.build_vocab(train_data)\n\nprint(f\"Number of training examples: {len(train_data)}\")\nprint(f\"Number of validation examples: {len(valid_data)}\")\nprint(f\"Number of testing examples: {len(test_data)}\")","metadata":{"id":"Yf1bw8XshS4n","execution":{"iopub.status.busy":"2022-03-27T06:07:50.91486Z","iopub.execute_input":"2022-03-27T06:07:50.915392Z","iopub.status.idle":"2022-03-27T06:10:05.732759Z","shell.execute_reply.started":"2022-03-27T06:07:50.915352Z","shell.execute_reply":"2022-03-27T06:10:05.731583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create iterator to sample batches from the dataset.","metadata":{"id":"7plvAExl-Xb6"}},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE, \n    device = device)","metadata":{"id":"NGJEvfD36dxS","execution":{"iopub.status.busy":"2022-03-27T06:10:05.733975Z","iopub.status.idle":"2022-03-27T06:10:05.734572Z","shell.execute_reply.started":"2022-03-27T06:10:05.734316Z","shell.execute_reply":"2022-03-27T06:10:05.734345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Model\n\nNext, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer.","metadata":{"id":"pnbV4_sbiMpH"}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\nbert = BertModel.from_pretrained('bert-base-uncased')","metadata":{"id":"vzgwu-L_iR-R","execution":{"iopub.status.busy":"2022-03-27T06:10:05.735766Z","iopub.status.idle":"2022-03-27T06:10:05.736356Z","shell.execute_reply.started":"2022-03-27T06:10:05.736112Z","shell.execute_reply":"2022-03-27T06:10:05.736138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll define our actual model. \n\nInstead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n\nWithin the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it.\nThe rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions.\n**When using a bidrectional GRU, we concatenate the final step of the forward and backward direction**","metadata":{"id":"WqvwnS-PiZr3"}},{"cell_type":"code","source":"from inspect import Parameter\nimport torch.nn as nn\n\nclass BERTGRUSentiment(nn.Module):\n    def __init__(self,\n                 bert,\n                 hidden_dim,\n                 output_dim,\n                 n_layers,\n                 bidirectional,\n                 dropout):\n        \"\"\"\n        Parameters: \n          bert: pre-trained BERT model\n          hidden_dim: hidden dimensionality of GRU\n          output_dim: output dimensionality of output linear layer (when non-bidirectional)\n          n_layers: number of GRU layers\n          bidirectional: True if GRU is bi-directional, False if otherwise.\n          dropout: dropout rate for the dropout layer\n        \"\"\"\n        super().__init__()\n\n        self.bert = bert\n        \n        # TODO: get the embedding dimension size 'hidden_size' from the transformer via its config attribute\n        embedding_dim = bert.config.hidden_size\n        \n        # TODO: add an n_layers GRU (you may use nn.GRU) - make sure to include kwargs 'bidirectional', 'batch_first=True', and 'dropout'     \n        self.rnn = nn.GRU(embedding_dim,hidden_dim,n_layers,batch_first=True,bidirectional=bidirectional,dropout=dropout)\n        \n        # TODO: add output linear layer (recall that we concatenate two hidden vectors when using bidirectional GRU)\n        self.out = nn.Linear(2*hidden_dim,output_dim) if bidirectional else nn.Linear(hidden_dim,output_dim)\n        \n        # TODO: add dropout layer\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n        # TODO: Compute the forward pass of the transformer inside a `torch.no_grad()` context.\n        with torch.no_grad():\n            embeds = self.bert(text)\n        # TODO: pass embeddings through recurrent network\n        hs,_ = self.rnn(embeds[\"last_hidden_state\"])\n        # TODO: Select the hidden state to use - last step for unidirectional -\n        # last step of forward and backward iteration concatenated for bidirectional\n        # (hint: look at the docs for nn.GRU - https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n        h = hs[:,-1,:]\n\n        # TODO: pass through dropout layer\n        h = self.dropout(h)\n        \n        # TODO: pass through output linear layer\n        output = self.out(h)\n        \n        return output","metadata":{"id":"BJJZCqi9ifsi","execution":{"iopub.status.busy":"2022-03-27T06:10:05.737483Z","iopub.status.idle":"2022-03-27T06:10:05.738078Z","shell.execute_reply.started":"2022-03-27T06:10:05.737821Z","shell.execute_reply":"2022-03-27T06:10:05.737859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we create an instance of our model. You need to select hyperparameters.\n\nIn order to freeze BERT paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. ","metadata":{"id":"9QIcv7DtkzC7"}},{"cell_type":"code","source":"################################### TODO ###################################\n# Adjust these hyperparameters as you see fit\n############################################################################\nHIDDEN_DIM = 32\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.25\nLEARNING_RATE = 1e-3\nN_EPOCHS = 3\n################################ END TODO ##################################\n\nmodel = BERTGRUSentiment(bert,\n                         HIDDEN_DIM,\n                         1,\n                         N_LAYERS,\n                         BIDIRECTIONAL,\n                         DROPOUT)\n\nfor name, param in model.named_parameters():                \n    if name.startswith('bert'):\n        param.requires_grad = False","metadata":{"id":"0sMepDxQeikM","execution":{"iopub.status.busy":"2022-03-27T06:10:05.73919Z","iopub.status.idle":"2022-03-27T06:10:05.739789Z","shell.execute_reply.started":"2022-03-27T06:10:05.739523Z","shell.execute_reply":"2022-03-27T06:10:05.739551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model\n\nAs is standard, we define our optimizer and criterion (loss function).","metadata":{"id":"FuZfESKVleH_"}},{"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.BCEWithLogitsLoss()\n\n# Place the model and criterion onto the GPU (if available)\nmodel = model.to(device)\ncriterion = criterion.to(device)","metadata":{"id":"0TylRmoglj6s","execution":{"iopub.status.busy":"2022-03-27T06:10:05.740919Z","iopub.status.idle":"2022-03-27T06:10:05.74147Z","shell.execute_reply.started":"2022-03-27T06:10:05.741234Z","shell.execute_reply":"2022-03-27T06:10:05.74126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes.","metadata":{"id":"p1f_kIrgltC3"}},{"cell_type":"code","source":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    # TODO: compute the binary_accuracy\n    preds = torch.sigmoid(preds)\n    preds[preds>0.5] = 1\n    preds[preds<=0.5] = 0\n    acc = ((preds==y).float().sum())/preds.shape[0]\n    return acc\n\ndef train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        predictions = model(batch.text).squeeze(1)\n        \n        loss = criterion(predictions, batch.label)\n        \n        acc = binary_accuracy(predictions, batch.label)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            predictions = model(batch.text).squeeze(1)\n            \n            loss = criterion(predictions, batch.label)\n            \n            acc = binary_accuracy(predictions, batch.label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\nimport time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"id":"VdAWegEVlvxh","execution":{"iopub.status.busy":"2022-03-27T06:10:05.742583Z","iopub.status.idle":"2022-03-27T06:10:05.743165Z","shell.execute_reply.started":"2022-03-27T06:10:05.742928Z","shell.execute_reply":"2022-03-27T06:10:05.742953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll train our model.\n\n**Please train your model such that it reaches 90% validation accuracy.** This is possible to accomplish within 15 minutes of training on GPU with the correct implementation and hyperparameters. Feel free to adjust the hyperparameters defined above in order to get the desired performance. Your points received will scale linearly from 0 for 50% accuracy to 8 for at least 90% accuracy.","metadata":{"id":"JMZ-1iKRmRtN"}},{"cell_type":"code","source":"best_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","metadata":{"id":"pUE1k-BZmVj1","execution":{"iopub.status.busy":"2022-03-27T06:10:05.74426Z","iopub.status.idle":"2022-03-27T06:10:05.744838Z","shell.execute_reply.started":"2022-03-27T06:10:05.744577Z","shell.execute_reply":"2022-03-27T06:10:05.744603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load up the parameters that gave us the best validation loss and try these on the test set","metadata":{"id":"YL4wDB-inioi"}},{"cell_type":"code","source":"model.load_state_dict(torch.load('best-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","metadata":{"id":"ogcFgM0gnjse","execution":{"iopub.status.busy":"2022-03-27T06:10:05.745936Z","iopub.status.idle":"2022-03-27T06:10:05.746492Z","shell.execute_reply.started":"2022-03-27T06:10:05.746255Z","shell.execute_reply":"2022-03-27T06:10:05.746282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nWe'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model. Feel free to add more test cases!","metadata":{"id":"fbIMuBOBnpci"}},{"cell_type":"code","source":"def predict_sentiment(model, tokenizer, sentence):\n    model.eval()\n    tokens = tokenizer.tokenize(sentence)\n    tokens = tokens[:max_input_length-2]\n    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(0)\n    prediction = torch.sigmoid(model(tensor))\n    return prediction.item()\n\nprint(predict_sentiment(model, tokenizer, \"This film is disgusting\"))\n\nprint(predict_sentiment(model, tokenizer, \"I hate this film\"))\n\nprint(predict_sentiment(model, tokenizer, \"I love this film\"))\n\nprint(predict_sentiment(model, tokenizer, \"I want watch this film again\"))\n\nprint(predict_sentiment(model, tokenizer, \"I can't wait to watch it again\"))\n\npredict_sentiment(model, tokenizer, \"The director is such a genius\")","metadata":{"id":"nAqpshDunvrB","execution":{"iopub.status.busy":"2022-03-27T06:10:05.747596Z","iopub.status.idle":"2022-03-27T06:10:05.748194Z","shell.execute_reply.started":"2022-03-27T06:10:05.747949Z","shell.execute_reply":"2022-03-27T06:10:05.747976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conceptual Questions\n\n\n1.   Why is the residual connection is crucial in the Transformer architecture? [5 points]The residual connections allow the gradients to flow directly through the network. And it also helps to keep some original information after transformations. It helps the gradient vanishing problem during training.","metadata":{"id":"oO386sdx0hRN"}},{"cell_type":"markdown","source":"2.   Why is Layer Normalization important in the Transformer architecture? [5 points]\n The Layer Normalizations can help stabilize the network thus the training process will be faster. And it is calculated for each instance independently, which makes it more suitable for data whose statistics across the batch dimension are often unstable (e.g. NLP data) than Batch norm.","metadata":{"id":"jbDAQO7gr6-H"}},{"cell_type":"markdown","source":"3.  Why do we use the scaling factor of $1/\\sqrt{d_k}$ in Scaled Dot Product Attention? If we remove it, what is going to happen? [5 points] The scaling factor helps normalization. For large value of $1/\\sqrt{d_k}$, the dot products $(QK^T)$ could explode and the softmax function will therefore have extremely small gradients. So we scale the dot products.","metadata":{"id":"7M79qzbksBfb"}},{"cell_type":"markdown","source":"# Submission PDF\n\nAs in assignment 1, please prepare a separate submission PDF for each problem. **Do not simply render your notebook as a pdf**. For this problem, please include the following in a PDF called `problem_2_solution.pdf`:\n\n1. Some short, one-sentence movie reviews that you wrote yourself, with your model's predicted sentiment.\n2. Answers to the conceptual questions above.\n\nNote that you still need to submit the jupyter notebook with all generated solutions.","metadata":{"id":"ytPPsGz2vBx0"}},{"cell_type":"markdown","source":"","metadata":{"id":"OVGrEwHXDUM3"}}]}